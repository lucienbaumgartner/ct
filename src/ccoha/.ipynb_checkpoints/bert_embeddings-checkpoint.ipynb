{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc74a7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88d609ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True,)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f94c141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_text_preparation(text, tokenizer):\n",
    "    \"\"\"\n",
    "    Preprocesses text input in a way that BERT can interpret.\n",
    "    \"\"\"\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1]*len(indexed_tokens)\n",
    "    # convert inputs to tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensor = torch.tensor([segments_ids])\n",
    "    return tokenized_text, tokens_tensor, segments_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a87f7307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(tokens_tensor, segments_tensor, model):\n",
    "    \"\"\"\n",
    "    Obtains BERT embeddings for tokens.\n",
    "    \"\"\"\n",
    "    # gradient calculation id disabled\n",
    "    with torch.no_grad():\n",
    "      # obtain hidden states\n",
    "        outputs = model(tokens_tensor, segments_tensor)\n",
    "        hidden_states = outputs[2]\n",
    "    # concatenate the tensors for all layers\n",
    "    # use \"stack\" to create new dimension in tensor\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    # remove dimension 1, the \"batches\"\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    # swap dimensions 0 and 1 so we can loop over tokens\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "    # intialized list to store embeddings\n",
    "    token_vecs_sum = []\n",
    "    # \"token_embeddings\" is a [Y x 12 x 768] tensor\n",
    "    # where Y is the number of tokens in the sentence\n",
    "    # loop over tokens in sentence\n",
    "    for token in token_embeddings:\n",
    "    # \"token\" is a [12 x 768] tensor\n",
    "    # sum the vectors from the last four layers\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "    return token_vecs_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6590aecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_embeddings(sentences):\n",
    "    context_embeddings = []\n",
    "    context_tokens = []\n",
    "    for sentence in sentences:\n",
    "        tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(sentence, tokenizer)\n",
    "        list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
    "        # make ordered dictionary to keep track of the position of each   word\n",
    "        tokens = OrderedDict()\n",
    "        # loop over tokens in sensitive sentence\n",
    "        for token in tokenized_text[1:-1]:\n",
    "            # keep track of position of word and whether it occurs multiple times\n",
    "            if token in tokens:\n",
    "                tokens[token] += 1\n",
    "            else:\n",
    "                tokens[token] = 1\n",
    "\n",
    "            # compute the position of the current token\n",
    "            token_indices = [i for i, t in enumerate(tokenized_text) if t == token]\n",
    "            current_index = token_indices[tokens[token]-1]\n",
    "            # get the corresponding embedding\n",
    "            token_vec = list_token_embeddings[current_index]\n",
    "\n",
    "            # save values\n",
    "            context_tokens.append(token)\n",
    "            context_embeddings.append(token_vec)\n",
    "        # add delimiter\n",
    "        context_tokens.append(\"@@@\")\n",
    "        context_embeddings.append(torch.tensor([0]).repeat(768))\n",
    "    return(context_embeddings, context_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e31fba44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentences_blame.txt',\n",
       " 'sentences_praise_lemmatized.txt',\n",
       " 'sentences_contrast_lemmatized.txt',\n",
       " 'sentences_blame_lemmatized.txt',\n",
       " 'sentences_contrast.txt']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_files = [f for f in os.listdir('../../output/ccohaa/data/') if re.match(r'sentences)', f)]\n",
    "txt_files[1:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d2bd500",
   "metadata": {},
   "outputs": [],
   "source": [
    "for txt_file in txt_files[1:6]:\n",
    "    df = pd.read_fwf('../../output/ccoha/data/' + txt_file, header = None, delimiter = \"@\")\n",
    "    sentences = df[0].tolist()\n",
    "    context_embeddings, context_tokens = wrap_embeddings(sentences)\n",
    "    filepath = os.path.join('../../output/ccoha/embeddings/')\n",
    "    name = 'metadata_' + txt_file.replace(\"txt\", \"tsv\")\n",
    "    with open(os.path.join(filepath, name), 'w+') as file_metadata:\n",
    "      for i, token in enumerate(context_tokens):\n",
    "        file_metadata.write(token + '\\n')\n",
    "\n",
    "    name = 'embeddings_' + txt_file.replace(\"txt\", \"tsv\")\n",
    "    with open(os.path.join(filepath, name), 'w+') as tsvfile:\n",
    "        writer = csv.writer(tsvfile, delimiter='\\t')\n",
    "        for embedding in context_embeddings:\n",
    "            writer.writerow(embedding.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
